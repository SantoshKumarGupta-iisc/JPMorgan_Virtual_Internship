{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from math import log\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "FILE_PATH = \"/content/Task 3 and 4_Loan_Data.csv\"\n",
        "TARGET_BUCKET_COUNT = 5 # Number of rating classes requested\n",
        "MIN_FICO = 300 # Standard minimum FICO score\n",
        "MAX_FICO = 850 # Standard maximum FICO score\n",
        "\n",
        "def load_and_prepare_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads data, extracts FICO scores and default status, and groups the data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return None\n",
        "\n",
        "    # 1. Prepare the key data points\n",
        "    # We only need FICO and Default status\n",
        "    df_fico = data[['fico_score', 'default']].copy()\n",
        "\n",
        "    # Ensure scores are within the standard range for clean processing\n",
        "    df_fico['fico_score'] = df_fico['fico_score'].clip(lower=MIN_FICO, upper=MAX_FICO)\n",
        "\n",
        "    # Group data by each unique FICO score to pre-calculate n_i and k_i\n",
        "    # This significantly speeds up the DP computation\n",
        "    grouped_data = df_fico.groupby('fico_score').agg(\n",
        "        n_i=('fico_score', 'size'), # total records (n_i)\n",
        "        k_i=('default', 'sum')      # total defaults (k_i)\n",
        "    ).reset_index()\n",
        "\n",
        "    # Create a mapping from FICO score to the row index for DP table access\n",
        "    fico_to_index = {score: i for i, score in enumerate(grouped_data['fico_score'])}\n",
        "\n",
        "    return grouped_data, fico_to_index\n",
        "\n",
        "def log_likelihood_contribution(k_i, n_i):\n",
        "    \"\"\"\n",
        "    Calculates the contribution to the log-likelihood function for a single bucket.\n",
        "\n",
        "    L = k_i * log(p_i) + (n_i - k_i) * log(1 - p_i)\n",
        "    Where p_i = k_i / n_i\n",
        "    \"\"\"\n",
        "    if n_i == 0:\n",
        "        return 0.0\n",
        "\n",
        "    p_i = k_i / n_i\n",
        "\n",
        "    # Avoid log(0) errors if p_i is 0 or 1, by adding a tiny epsilon\n",
        "    epsilon = 1e-9\n",
        "    p_i = np.clip(p_i, epsilon, 1.0 - epsilon)\n",
        "\n",
        "    log_L = k_i * log(p_i) + (n_i - k_i) * log(1.0 - p_i)\n",
        "    return log_L\n",
        "\n",
        "def solve_quantization_dp(grouped_data, K):\n",
        "    \"\"\"\n",
        "    Implements Dynamic Programming to find K optimal bucket boundaries\n",
        "    that maximize the total log-likelihood.\n",
        "\n",
        "    Args:\n",
        "        grouped_data (pd.DataFrame): Data grouped by FICO score.\n",
        "        K (int): The target number of buckets.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (Optimal boundaries list, Optimal log-likelihood score).\n",
        "    \"\"\"\n",
        "    N = len(grouped_data) # Total number of unique FICO groups\n",
        "\n",
        "    # --- Step 1: Pre-calculate the cost (Log-Likelihood) for all possible single intervals ---\n",
        "    # cost[i][j] stores the max log-likelihood for one bucket covering groups i through j\n",
        "    cost = np.zeros((N, N))\n",
        "\n",
        "    for i in range(N):\n",
        "        current_k = 0\n",
        "        current_n = 0\n",
        "        for j in range(i, N):\n",
        "            current_k += grouped_data.loc[j, 'k_i']\n",
        "            current_n += grouped_data.loc[j, 'n_i']\n",
        "            cost[i][j] = log_likelihood_contribution(current_k, current_n)\n",
        "\n",
        "    # --- Step 2: Dynamic Programming Table Setup ---\n",
        "    # DP[k][j]: Maximum log-likelihood using 'k' buckets ending at group 'j'\n",
        "    DP = np.full((K + 1, N), -np.inf)\n",
        "    # P[k][j]: The index 'i' of the optimal split point for DP[k][j]\n",
        "    P = np.zeros((K + 1, N), dtype=int)\n",
        "\n",
        "    # Base Case: 1 bucket (k=1)\n",
        "    for j in range(N):\n",
        "        DP[1][j] = cost[0][j]\n",
        "        P[1][j] = 0\n",
        "\n",
        "    # DP Recurrence: k buckets ending at j\n",
        "    for k in range(2, K + 1):\n",
        "        for j in range(k - 1, N): # Must have at least k groups for k buckets\n",
        "            for i in range(k - 2, j): # Previous split point i\n",
        "                # DP[k][j] = max_{i < j} ( DP[k-1][i] + cost[i+1][j] )\n",
        "                current_value = DP[k-1][i] + cost[i+1][j]\n",
        "\n",
        "                if current_value > DP[k][j]:\n",
        "                    DP[k][j] = current_value\n",
        "                    P[k][j] = i\n",
        "\n",
        "    # --- Step 3: Backtrack to find optimal boundaries ---\n",
        "\n",
        "    # Start at the end: K buckets ending at the last group (N-1)\n",
        "    boundaries_indices = []\n",
        "    current_index = N - 1\n",
        "\n",
        "    for k in range(K, 1, -1):\n",
        "        # The split point index 'i' gives the end of the previous bucket (k-1)\n",
        "        # The new bucket starts at index i + 1\n",
        "        split_point_index = P[k][current_index]\n",
        "\n",
        "        # The boundary FICO score is between grouped_data[i]['fico_score'] and grouped_data[i+1]['fico_score']\n",
        "        # We store the index of the score that STARTS the current bucket (i+1)\n",
        "        boundaries_indices.append(split_point_index + 1)\n",
        "\n",
        "        # Move to the end of the previous optimal interval\n",
        "        current_index = split_point_index\n",
        "\n",
        "    # Reverse the indices to get them in ascending order\n",
        "    boundaries_indices = sorted(boundaries_indices)\n",
        "\n",
        "    # Convert group indices back to FICO scores (the boundary is the *start* of the new bucket)\n",
        "    optimal_boundaries = [grouped_data.loc[idx, 'fico_score'] for idx in boundaries_indices]\n",
        "\n",
        "    # Add the overall min and max FICO scores for clarity\n",
        "    final_boundaries = [MIN_FICO] + optimal_boundaries + [MAX_FICO + 1]\n",
        "\n",
        "    return final_boundaries, DP[K][N-1]\n",
        "\n",
        "def create_rating_map(boundaries, grouped_data):\n",
        "    \"\"\"\n",
        "    Creates a rating map that assigns a rating (1=Best, K=Worst) to FICO score ranges.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the final rating map by iterating through the groups\n",
        "    rating_map = {}\n",
        "\n",
        "    # K is the number of buckets, which is len(boundaries) - 1\n",
        "    num_buckets = len(boundaries) - 1\n",
        "\n",
        "    # Determine the credit rating for each bucket. Lower rating = Better Score.\n",
        "    # Since FICO is sorted, the first bucket has the lowest scores (worst credit).\n",
        "    # We need to map the best FICO score range (highest scores) to rating 1.\n",
        "\n",
        "    # First, calculate the PD for each bucket\n",
        "    pd_per_bucket = []\n",
        "    for i in range(num_buckets):\n",
        "        lower_bound = boundaries[i]\n",
        "        upper_bound = boundaries[i+1]\n",
        "\n",
        "        # Filter the original grouped data for the current bucket\n",
        "        bucket_data = grouped_data[\n",
        "            (grouped_data['fico_score'] >= lower_bound) &\n",
        "            (grouped_data['fico_score'] < upper_bound)\n",
        "        ]\n",
        "\n",
        "        n_i = bucket_data['n_i'].sum()\n",
        "        k_i = bucket_data['k_i'].sum()\n",
        "\n",
        "        pd_per_bucket.append({\n",
        "            'min_score': lower_bound,\n",
        "            'max_score': upper_bound - 1,\n",
        "            'default_rate': k_i / n_i if n_i > 0 else 0,\n",
        "            'total_count': n_i,\n",
        "        })\n",
        "\n",
        "    # Sort the buckets by default rate (ascending) to assign ratings\n",
        "    # Lowest default rate gets the best rating (Rating 1)\n",
        "    pd_per_bucket.sort(key=lambda x: x['default_rate'], reverse=False)\n",
        "\n",
        "    # Assign the final rating based on the sorted order\n",
        "    # Rating 1 is the best (lowest PD)\n",
        "    for i, bucket in enumerate(pd_per_bucket):\n",
        "        # We need to assign a numerical rating from 1 (best) to K (worst)\n",
        "        bucket['rating'] = i + 1\n",
        "\n",
        "    # Convert the rating map back to FICO score order for presentation\n",
        "    # Sort by FICO score range (min_score)\n",
        "    pd_per_bucket.sort(key=lambda x: x['min_score'])\n",
        "\n",
        "    return pd_per_bucket\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution logic.\n",
        "    \"\"\"\n",
        "    print(f\"Starting FICO Quantization for K={TARGET_BUCKET_COUNT} buckets...\")\n",
        "\n",
        "    # Step 1: Load and Prepare Data\n",
        "    grouped_data, fico_to_index = load_and_prepare_data(FILE_PATH)\n",
        "    if grouped_data is None:\n",
        "        return\n",
        "\n",
        "    # Step 2: Solve the Dynamic Programming problem\n",
        "    boundaries, max_log_L = solve_quantization_dp(grouped_data, TARGET_BUCKET_COUNT)\n",
        "\n",
        "    # Step 3: Create the Rating Map\n",
        "    rating_map = create_rating_map(boundaries, grouped_data)\n",
        "\n",
        "    # --- Display Results ---\n",
        "\n",
        "    # Calculate the exact score where the boundary falls (the first score of the new bucket)\n",
        "    boundary_scores = [s for s in boundaries if s > MIN_FICO and s <= MAX_FICO]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Optimal FICO Boundaries for K={TARGET_BUCKET_COUNT} (Max Log-Likelihood)\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Maximum Log-Likelihood Achieved: {max_log_L:.2f}\")\n",
        "    print(f\"Optimal Score Splits (Start of New Bucket): {boundary_scores}\")\n",
        "\n",
        "    # Create the final output table\n",
        "    final_output = []\n",
        "\n",
        "    # Use the boundaries list to create the final credit rating bands\n",
        "    for i in range(len(boundaries) - 1):\n",
        "        min_score = boundaries[i]\n",
        "        max_score = boundaries[i+1] - 1\n",
        "\n",
        "        # Find the corresponding PD and Rating from the map\n",
        "        bucket_info = next(item for item in rating_map if item['min_score'] == min_score)\n",
        "\n",
        "        final_output.append({\n",
        "            \"Rating\": bucket_info['rating'],\n",
        "            \"FICO Range\": f\"{min_score}-{max_score}\",\n",
        "            \"Default Rate (PD)\": f\"{bucket_info['default_rate']:.4f}\",\n",
        "            \"Count\": bucket_info['total_count']\n",
        "        })\n",
        "\n",
        "    # Sort the final output by FICO Range (ascending)\n",
        "    final_output.sort(key=lambda x: int(x['FICO Range'].split('-')[0]))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*20 + \" FICO RATING MAP \" + \"=\"*21)\n",
        "    print(\" (Lower Rating signifies Better Creditworthiness)\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Print the final result table\n",
        "    df_result = pd.DataFrame(final_output)\n",
        "    print(df_result.to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
        "    print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FICO Quantization for K=5 buckets...\n",
            "\n",
            "==================================================\n",
            "Optimal FICO Boundaries for K=5 (Max Log-Likelihood)\n",
            "==================================================\n",
            "Maximum Log-Likelihood Achieved: -4255.38\n",
            "Optimal Score Splits (Start of New Bucket): [np.int64(521), np.int64(581), np.int64(641), np.int64(697)]\n",
            "\n",
            "==================== FICO RATING MAP =====================\n",
            " (Lower Rating signifies Better Creditworthiness)\n",
            "==================================================\n",
            "| Rating   | FICO Range   | Default Rate (PD)   | Count   |\n",
            "|:---------|:-------------|:--------------------|:--------|\n",
            "| 5        | 300-520      | 0.6611              | 301     |\n",
            "| 4        | 521-580      | 0.381               | 1407    |\n",
            "| 3        | 581-640      | 0.2045              | 3438    |\n",
            "| 2        | 641-696      | 0.1051              | 3197    |\n",
            "| 1        | 697-850      | 0.0465              | 1657    |\n",
            "==================================================\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9rYbP2tUeIu",
        "outputId": "c7e339af-03d0-41d2-a60a-388f81b57dd6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yWzVy82mUhtA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}